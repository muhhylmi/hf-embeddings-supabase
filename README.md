// README.md

# LLM RAG with Supabase pgvector (TypeScript + Express)

# ğŸ§  RAG App - Supabase + Hugging Face + Groq

A simple Retrieval-Augmented Generation (RAG) app using:

-   **Supabase** â†’ Vector database for semantic search
-   **Hugging Face** â†’ Free embedding model (`all-MiniLM-L6-v2`)
-   **Groq** â†’ LLM inference with LLaMA 3 for natural language answers

---

## ğŸš€ Features

-   Ingest long text automatically **with chunking**.
-   Store embeddings in **Supabase**.
-   Perform **semantic search** using Supabase.
-   Use **Groq** LLaMA 3 to answer questions based on search results.

---

ğŸ“Š Flow of the App

-   User Input Question â†’ User asks a question.
-   Hugging Face â†’ Generate Embedding â†’ Convert question into vector form.
-   Supabase â†’ Search Relevant Chunks â†’ Find most similar text chunks.
-   Groq LLaMA 3 â†’ Generate Final Answer â†’ Create a natural language answer.
-   Return Answer to User â†’ Display final output.
    User â¡ï¸ Embedding â¡ï¸ Supabase Search â¡ï¸ LLM Groq â¡ï¸ Answer

Quickstart:

1. Copy `.env.example` to `.env` and fill values (OpenAI key, Supabase URL & service role key).
2. Run SQL in Supabase SQL Editor: `SQL/setup.sql` contents (creates `documents` table + RPC function).
3. Install deps: `npm install`.
4. Start dev server: `npm run dev`.
5. Ingest: POST `/ingest` with `{ "text": "..." }`.
6. Chat: POST `/chat` with `{ "question": "..." }`.

Notes:

-   Use SUPABASE service role key on the server to allow RPC that takes vector as param.
-   Adjust `VECTOR_DIM` in `.env` and SQL if you use a different embedding model.
-   For production, secure the server and don't leak the service role key.

---
